# -*- coding: utf-8 -*-
"""
çŸ¥è¯†å›¾è°±æ„å»ºå™¨
ä½¿ç”¨ç›˜å¤7Båˆ†æé¢˜ç›®ï¼Œæ„å»ºå…¨å±€çŸ¥è¯†å›¾è°±
"""

import logging
import json
import re
import networkx as nx
from typing import List, Dict, Any, Optional
from pathlib import Path
import pickle

logger = logging.getLogger(__name__)


class KnowledgeGraphBuilder:
    """çŸ¥è¯†å›¾è°±æ„å»ºå™¨ - ä½¿ç”¨ç›˜å¤7B"""
    
    def __init__(self, llm_model, cache_path: str = "./data/knowledge_graph.pkl"):
        """
        Args:
            llm_model: ç›˜å¤7Bæ¨¡å‹
            cache_path: å›¾è°±ç¼“å­˜è·¯å¾„
        """
        self.llm_model = llm_model
        self.cache_path = Path(cache_path)
        self.graph = nx.DiGraph()  # æœ‰å‘å›¾
        
        logger.info("âœ… çŸ¥è¯†å›¾è°±æ„å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def build_from_questions(self, questions: List[Dict[str, Any]], 
                           force_rebuild: bool = False) -> nx.DiGraph:
        """
        ä»é¢˜ç›®æ„å»ºçŸ¥è¯†å›¾è°±
        
        Args:
            questions: é¢˜ç›®åˆ—è¡¨
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
            
        Returns:
            çŸ¥è¯†å›¾è°±ï¼ˆNetworkXå›¾ï¼‰
        """
        # æ£€æŸ¥ç¼“å­˜
        if not force_rebuild and self.cache_path.exists():
            logger.info("ğŸ“‚ ä»ç¼“å­˜åŠ è½½çŸ¥è¯†å›¾è°±...")
            return self._load_from_cache()
        
        logger.info(f"ğŸ”¨ å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆå…± {len(questions)} é“é¢˜ï¼‰...")
        
        # ç¡®ä¿ç›˜å¤7Bå·²åŠ è½½
        if not self.llm_model.is_loaded:
            logger.info("ğŸ”„ åŠ è½½ç›˜å¤7Bæ¨¡å‹...")
            self.llm_model.load_model()
        
        # 1. æ·»åŠ åŸºç¡€èŠ‚ç‚¹ï¼ˆé¢˜ç›®ã€çŸ¥è¯†ç‚¹ï¼‰
        self._add_basic_nodes(questions)
        
        # 2. ä½¿ç”¨ç›˜å¤7Bæ‰¹é‡åˆ†æé¢˜ç›®ï¼Œæå–æ·±å±‚å…³ç³»
        self._extract_relations_with_llm(questions)
        
        # 3. ä¿å­˜ç¼“å­˜
        self._save_to_cache()
        
        logger.info(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: {self.graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹, "
                   f"{self.graph.number_of_edges()} æ¡è¾¹")
        
        return self.graph
    
    def _add_basic_nodes(self, questions: List[Dict[str, Any]]):
        """æ·»åŠ åŸºç¡€èŠ‚ç‚¹å’Œå…³ç³»"""
        logger.info("ğŸ“ æ·»åŠ åŸºç¡€èŠ‚ç‚¹...")
        
        for q in questions:
            q_id = q.get('é¢˜å·')
            major = q.get('çŸ¥è¯†ç‚¹å¤§ç±»', q.get('knowledge_point_major', 'æœªçŸ¥'))
            minor = q.get('çŸ¥è¯†ç‚¹å°ç±»', q.get('knowledge_point_minor', 'æœªçŸ¥'))
            difficulty = q.get('éš¾åº¦', 0.5)
            
            # æ·»åŠ é¢˜ç›®èŠ‚ç‚¹
            self.graph.add_node(
                f"Q{q_id}",
                type='question',
                data=q,
                major_point=major,
                minor_point=minor,
                difficulty=difficulty
            )
            
            # æ·»åŠ çŸ¥è¯†ç‚¹èŠ‚ç‚¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            major_node = f"KP_Major:{major}"
            minor_node = f"KP_Minor:{major}/{minor}"
            
            if not self.graph.has_node(major_node):
                self.graph.add_node(major_node, type='major_point', name=major)
            
            if not self.graph.has_node(minor_node):
                self.graph.add_node(minor_node, type='minor_point', 
                                  major=major, minor=minor, name=f"{major}/{minor}")
            
            # æ·»åŠ åŸºç¡€å…³ç³»
            self.graph.add_edge(minor_node, major_node, relation='belongs_to')
            self.graph.add_edge(f"Q{q_id}", minor_node, relation='tests', 
                              difficulty=difficulty)
        
        logger.info(f"âœ… åŸºç¡€èŠ‚ç‚¹æ·»åŠ å®Œæˆ: {self.graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹")
    
    def _extract_relations_with_llm(self, questions: List[Dict[str, Any]]):
        """ä½¿ç”¨ç›˜å¤7Bæå–æ·±å±‚çŸ¥è¯†å…³ç³»"""
        logger.info("ğŸ¤– ä½¿ç”¨ç›˜å¤7Båˆ†æé¢˜ç›®ï¼Œæå–çŸ¥è¯†å…³ç³»...")
        
        # æŒ‰çŸ¥è¯†ç‚¹åˆ†ç»„
        kp_groups = {}
        for q in questions:
            major = q.get('çŸ¥è¯†ç‚¹å¤§ç±»', q.get('knowledge_point_major', 'æœªçŸ¥'))
            minor = q.get('çŸ¥è¯†ç‚¹å°ç±»', q.get('knowledge_point_minor', 'æœªçŸ¥'))
            key = f"{major}/{minor}"
            if key not in kp_groups:
                kp_groups[key] = []
            kp_groups[key].append(q)
        
        # æ¯ä¸ªçŸ¥è¯†ç‚¹åˆ†æä¸€æ¬¡ï¼ˆé¿å…é‡å¤è°ƒç”¨LLMï¼‰
        total_groups = len(kp_groups)
        for idx, (kp_key, kp_questions) in enumerate(kp_groups.items(), 1):
            logger.info(f"ğŸ” åˆ†æçŸ¥è¯†ç‚¹ {idx}/{total_groups}: {kp_key}")
            
            # æ„å»ºè¯¥çŸ¥è¯†ç‚¹çš„ä¸Šä¸‹æ–‡ï¼ˆæœ€å¤š3é“é¢˜ï¼‰
            sample_questions = kp_questions[:3]
            context = self._build_context(sample_questions)
            
            # è°ƒç”¨ç›˜å¤7Bæå–å…³ç³»
            relations = self._call_llm_for_relations(kp_key, context)
            
            # æ·»åŠ åˆ°å›¾è°±
            self._add_relations_to_graph(kp_key, relations)
        
        logger.info(f"âœ… çŸ¥è¯†å…³ç³»æå–å®Œæˆ: {self.graph.number_of_edges()} æ¡è¾¹")
    
    def _build_context(self, questions: List[Dict[str, Any]]) -> str:
        """æ„å»ºåˆ†æä¸Šä¸‹æ–‡"""
        context_parts = []
        for q in questions:
            context_parts.append(f"""
é¢˜ç›®: {q.get('é—®é¢˜', '')[:100]}
ç­”æ¡ˆ: {q.get('ç­”æ¡ˆ', '')[:50]}
è§£æ: {q.get('è§£æ', '')[:100]}
""")
        return "\n".join(context_parts)
    
    def _call_llm_for_relations(self, kp_key: str, context: str) -> Dict[str, Any]:
        """è°ƒç”¨ç›˜å¤7Bæå–çŸ¥è¯†å…³ç³»"""
        prompt = f"""åˆ†æä»¥ä¸‹æ•°å­¦çŸ¥è¯†ç‚¹çš„é¢˜ç›®ï¼Œè¯†åˆ«å…³é”®æ¦‚å¿µå’Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚

çŸ¥è¯†ç‚¹: {kp_key}

é¢˜ç›®ç¤ºä¾‹:
{context}

è¯·æå–ï¼š
1. æ ¸å¿ƒæ¦‚å¿µï¼ˆ3-5ä¸ªå…³é”®è¯ï¼‰
2. å‰ç½®çŸ¥è¯†ï¼ˆå­¦ä¹ æ­¤çŸ¥è¯†ç‚¹å‰éœ€è¦æŒæ¡çš„ï¼‰
3. åç»­çŸ¥è¯†ï¼ˆæŒæ¡æ­¤çŸ¥è¯†ç‚¹åå¯ä»¥å­¦ä¹ çš„ï¼‰
4. è§£é¢˜æ–¹æ³•ï¼ˆå¸¸ç”¨çš„æ–¹æ³•æˆ–æŠ€å·§ï¼‰

ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "concepts": ["æ¦‚å¿µ1", "æ¦‚å¿µ2", "æ¦‚å¿µ3"],
  "prerequisites": ["å‰ç½®çŸ¥è¯†1", "å‰ç½®çŸ¥è¯†2"],
  "next_topics": ["åç»­çŸ¥è¯†1", "åç»­çŸ¥è¯†2"],
  "methods": ["æ–¹æ³•1", "æ–¹æ³•2"]
}}

åªè¾“å‡ºJSONï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–æ–‡å­—ã€‚"""
        
        try:
            response = self.llm_model.generate(
                prompt,
                temperature=0.1,
                max_length=512,
                enable_thinking=False
            )
            
            # è§£æJSON
            return self._parse_llm_response(response)
            
        except Exception as e:
            logger.warning(f"âš ï¸  LLMåˆ†æå¤±è´¥: {e}")
            return {'concepts': [], 'prerequisites': [], 'next_topics': [], 'methods': []}
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """è§£æç›˜å¤7Bçš„JSONå“åº”"""
        try:
            # æå–JSONéƒ¨åˆ†
            start = response.find('{')
            end = response.rfind('}') + 1
            if start != -1 and end > start:
                json_str = response[start:end]
                return json.loads(json_str)
        except Exception as e:
            logger.warning(f"âš ï¸  JSONè§£æå¤±è´¥: {e}")
        
        # é™çº§ï¼šæ­£åˆ™æå–
        result = {
            'concepts': self._extract_list(response, r'æ¦‚å¿µ[:ï¼š]([^\n]+)'),
            'prerequisites': self._extract_list(response, r'å‰ç½®[:ï¼š]([^\n]+)'),
            'next_topics': self._extract_list(response, r'åç»­[:ï¼š]([^\n]+)'),
            'methods': self._extract_list(response, r'æ–¹æ³•[:ï¼š]([^\n]+)')
        }
        return result
    
    def _extract_list(self, text: str, pattern: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–åˆ—è¡¨"""
        matches = re.findall(pattern, text)
        if matches:
            items = matches[0].split('ã€')
            return [item.strip() for item in items if item.strip()]
        return []
    
    def _add_relations_to_graph(self, kp_key: str, relations: Dict[str, Any]):
        """å°†æå–çš„å…³ç³»æ·»åŠ åˆ°å›¾è°±"""
        minor_node = f"KP_Minor:{kp_key}"
        
        # æ·»åŠ æ¦‚å¿µèŠ‚ç‚¹
        for concept in relations.get('concepts', []):
            concept_node = f"Concept:{concept}"
            if not self.graph.has_node(concept_node):
                self.graph.add_node(concept_node, type='concept', name=concept)
            self.graph.add_edge(minor_node, concept_node, relation='involves')
        
        # æ·»åŠ å‰ç½®çŸ¥è¯†å…³ç³»
        for prereq in relations.get('prerequisites', []):
            prereq_node = f"KP_Minor:{prereq}"
            if self.graph.has_node(prereq_node):
                self.graph.add_edge(prereq_node, minor_node, relation='prerequisite')
        
        # æ·»åŠ åç»­çŸ¥è¯†å…³ç³»
        for next_topic in relations.get('next_topics', []):
            next_node = f"KP_Minor:{next_topic}"
            if self.graph.has_node(next_node):
                self.graph.add_edge(minor_node, next_node, relation='leads_to')
        
        # æ·»åŠ æ–¹æ³•èŠ‚ç‚¹
        for method in relations.get('methods', []):
            method_node = f"Method:{method}"
            if not self.graph.has_node(method_node):
                self.graph.add_node(method_node, type='method', name=method)
            self.graph.add_edge(minor_node, method_node, relation='uses')
    
    def _save_to_cache(self):
        """ä¿å­˜å›¾è°±åˆ°ç¼“å­˜"""
        try:
            self.cache_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.cache_path, 'wb') as f:
                pickle.dump(self.graph, f)
            logger.info(f"ğŸ’¾ çŸ¥è¯†å›¾è°±å·²ç¼“å­˜åˆ°: {self.cache_path}")
        except Exception as e:
            logger.warning(f"âš ï¸  ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def _load_from_cache(self) -> nx.DiGraph:
        """ä»ç¼“å­˜åŠ è½½å›¾è°±"""
        try:
            with open(self.cache_path, 'rb') as f:
                self.graph = pickle.load(f)
            logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½çŸ¥è¯†å›¾è°±: {self.graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹, "
                       f"{self.graph.number_of_edges()} æ¡è¾¹")
            return self.graph
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            raise
    
    def get_graph(self) -> nx.DiGraph:
        """è·å–çŸ¥è¯†å›¾è°±"""
        return self.graph


def create_kg_builder(llm_model, cache_path: str = "./data/knowledge_graph.pkl"):
    """åˆ›å»ºçŸ¥è¯†å›¾è°±æ„å»ºå™¨"""
    return KnowledgeGraphBuilder(llm_model, cache_path)